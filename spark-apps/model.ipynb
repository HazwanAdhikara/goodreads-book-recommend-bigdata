{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f241792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import functions as F, SparkSession\n",
    "from pyspark.sql.functions import concat_ws, split, col\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer, StopWordsRemover,\n",
    "    CountVectorizer\n",
    ")\n",
    "from pyspark.ml.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7a3e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:21:11 WARN Utils: Your hostname, Zwanes-MacBook.local resolves to a loopback address: 127.0.0.1; using 192.168.110.53 instead (on interface en0)\n",
      "25/06/26 22:21:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/26 22:21:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GoodreadsBookModel\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8d60701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Reading all CSVs from ../streamlit-app/data/*.csv â€¦\n",
      "Found 37 CSV files:\n",
      "  - ../streamlit-app/data/books_batch_015_20250620_033009.csv\n",
      "  - ../streamlit-app/data/books_batch_014_20250620_033008.csv\n",
      "  - ../streamlit-app/data/books_batch_022_20250620_033015.csv\n",
      "  - ../streamlit-app/data/books_batch_021_20250620_033014.csv\n",
      "  - ../streamlit-app/data/books_batch_037_20250620_033108_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Combined DataFrame has 183,780 rows and 11 columns\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Authors: string (nullable = true)\n",
      " |-- Rating: string (nullable = true)\n",
      " |-- PublishYear: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- RatingDistTotal: string (nullable = true)\n",
      " |-- PagesNumber: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- batch_id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir = \"../streamlit-app/data/\"\n",
    "csv_pattern = os.path.join(data_dir, \"*.csv\")\n",
    "\n",
    "print(f\"â–¶ï¸ Reading all CSVs from {csv_pattern} â€¦\")\n",
    "\n",
    "# Check if files exist\n",
    "import glob\n",
    "csv_files = glob.glob(csv_pattern)\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files[:5]:  # Show first 5 files\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "if csv_files:\n",
    "    df = spark.read.csv(csv_pattern, header=True, inferSchema=True)\n",
    "    print(f\"âœ… Combined DataFrame has {df.count():,} rows and {len(df.columns)} columns\")\n",
    "    df.printSchema()\n",
    "else:\n",
    "    print(\"âŒ No CSV files found! Make sure Streamlit has generated batch files.\")\n",
    "    print(\"Please run the Streamlit app in 'Full Dataset Processing' mode first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005bcca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=55680) is multi-threaded, use of fork() may lead to deadlocks in the child.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+---------------------+\n",
      "|Column     |Missing Values|Percentage           |\n",
      "+-----------+--------------+---------------------+\n",
      "|Description|5             |0.0027206442485580586|\n",
      "|timestamp  |1             |5.441288497116117E-4 |\n",
      "|batch_id   |1             |5.441288497116117E-4 |\n",
      "+-----------+--------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "\n",
    "missing_counts = df.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "summary_rows = [\n",
    "    (col, cnt, cnt / total_rows * 100)\n",
    "    for col, cnt in missing_counts.items()\n",
    "    if cnt > 0\n",
    "]\n",
    "missing_summary_df = spark.createDataFrame(\n",
    "    summary_rows,\n",
    "    schema=[\"Column\", \"Missing Values\", \"Percentage\"]\n",
    ").orderBy(F.desc(\"Missing Values\"))\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "missing_summary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba1a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ After cleaning: 183,775 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [\"batch_id\", \"timestamp\", \"RatingDistTotal\"]\n",
    "df_clean = df.drop(*cols_to_drop)\n",
    "df_clean = df_clean.dropna(how=\"any\", subset=[\"Description\"])\n",
    "print(f\"â–¶ï¸ After cleaning: {df_clean.count():,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5694f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found in df_clean.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_rows_clean = df_clean.count()\n",
    "\n",
    "missing_counts_clean = df_clean.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "summary_rows_clean = [\n",
    "    (col, cnt, cnt / total_rows_clean * 100)\n",
    "    for col, cnt in missing_counts_clean.items()\n",
    "    if cnt > 0\n",
    "]\n",
    "if summary_rows_clean:\n",
    "    missing_summary_df_clean = spark.createDataFrame(\n",
    "        summary_rows_clean,\n",
    "        schema=[\"Column\", \"Missing Values\", \"Percentage\"]\n",
    "    ).orderBy(F.desc(\"Missing Values\"))\n",
    "    print(\"Missing Values Summary for df_clean:\")\n",
    "    missing_summary_df_clean.show(truncate=False)\n",
    "else:\n",
    "    print(\"No missing values found in df_clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1824a81",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78fd3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:21:37 WARN StopWordsRemover: Default locale set was [en_ID]; however, it was not found in available locales in JVM, falling back to en_US locale. Set param `locale` in order to respect another locale.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ï¸ Sample features:\n",
      "+-------+--------------------------------------------------+--------------------------------------------------+\n",
      "|     Id|                                              Name|                                          features|\n",
      "+-------+--------------------------------------------------+--------------------------------------------------+\n",
      "|4411486|      Larousse Dictionary of Beliefs and Religions|(10000,[1,11,16,19,22,59,66,114,133,134,135,152...|\n",
      "|4411489|Enhancing Industrial Performance: Experiences O...|(10000,[1,2,3,7,43,45,76,85,91,172,234,356,389,...|\n",
      "|4411490|   The Brass Check: A Study of American Journalism|(10000,[3,6,9,10,42,65,118,126,354,381,501,560,...|\n",
      "|4411491| Transient Temperatures in Engineering and Science|(10000,[0,2,10,14,68,76,88,90,95,142,156,184,29...|\n",
      "|4411496|                   Citroen 2CV: The Complete Story|(10000,[3,17,26,37,103,259,280,360,398,460,478,...|\n",
      "+-------+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"Description\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "df_tok = tokenizer.transform(df_clean)\n",
    "remover  = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_tok   = remover.transform(df_tok)\n",
    "\n",
    "df_tags = df_tok.withColumn(\n",
    "    \"tags\",\n",
    "    concat_ws(\" \",\n",
    "        col(\"filtered_words\"), col(\"Authors\"), col(\"Publisher\"),\n",
    "        col(\"Rating\").cast(\"string\"), col(\"PublishYear\").cast(\"string\"), col(\"PagesNumber\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "df_tags = df_tags.withColumn(\"tags\", split(col(\"tags\"), \" \"))\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tags\", outputCol=\"features\", vocabSize=10000, minDF=2)\n",
    "cv_model = cv.fit(df_tags)\n",
    "df_feat  = cv_model.transform(df_tags)\n",
    "print(\"â–¶ï¸ Sample features:\")\n",
    "df_feat.select(\"Id\",\"Name\",\"features\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d022124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|     Id|                                              Name|                                       Description|                                             words|                                    filtered_words|\n",
      "+-------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|4411486|      Larousse Dictionary of Beliefs and Religions|More comprehensive than any other single volume...|[more, comprehensive, than, any, other, single,...|[comprehensive, single, volume, reference, subj...|\n",
      "|4411489|Enhancing Industrial Performance: Experiences O...|This work focuses on the implementation of soci...|[this, work, focuses, on, the, implementation, ...|[work, focuses, implementation, socio, technica...|\n",
      "|4411490|   The Brass Check: A Study of American Journalism|\"\"\"In this systematic critique of the structura...|[in, this, systematic, critique, of, the, struc...|[systematic, critique, structural, basis, u, me...|\n",
      "|4411491| Transient Temperatures in Engineering and Science|For anyone involved in mechanical or chemical e...|[for, anyone, involved, in, mechanical, or, che...|[anyone, involved, mechanical, chemical, engine...|\n",
      "|4411496|                   Citroen 2CV: The Complete Story|The Citroen 2CV is a true classic and deserves ...|[the, citroen, 2cv, is, a, true, classic, and, ...|[citroen, 2cv, true, classic, deserves, place, ...|\n",
      "+-------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id     |Name                                                                         |tags                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|4411486|Larousse Dictionary of Beliefs and Religions                                 |[comprehensive, single, volume, reference, subject, dictionary, covers, world, major, religions, beliefs, current, historical, 2, 900, concise, informative, entries, dictionary, spans, periods, places, provides, coverage, today, hinduism, christianity, judaism, islam, along, rites, ancient, civilizations, well, less, mainstream, beliefs, scientology, pantheism, supplements, include, subject, index, calendar, festivals, chart, detailing, population, distribution, major, faiths, Frank, Whaling, Larousse, Kingfisher, Chambers, 0.0, 1994, 605]|\n",
      "|4411489|Enhancing Industrial Performance: Experiences Of Integrating The Human Factor|[work, focuses, implementation, socio, technical, innovation, manufacturing, companies, offering, practical, examples, management, human, computer, interface, example, includes, cost, benefit, analysis, book, adopts, Harman, Kragt, CRC, Press, 0.0, 1992, 330]                                                                                                                                                                                                                                                                                              |\n",
      "|4411490|The Brass Check: A Study of American Journalism                              |[systematic, critique, structural, basis, u, media, arguably, first, one, ever, published, upton, sinclair, writes, american, journalism, class, institution, serving, rich, spurning, poor, likening, journalists, prostitutes, Upton, Sinclair, University, of, Illinois, Press, 3.81, 2002, 480]                                                                                                                                                                                                                                                              |\n",
      "|4411491|Transient Temperatures in Engineering and Science                            |[anyone, involved, mechanical, chemical, engineering, experimental, physics, chemistry, techniques, available, measurement, transient, temperatures, heat, flows, described, book, emphasis, practical, experimental, techniques, applications, various, numerical, methods, analyzing, results, also, included, br, B., Lawton, Oxford, University, Press,, USA, 5.0, 1996, 600]                                                                                                                                                                                |\n",
      "|4411496|Citroen 2CV: The Complete Story                                              |[citroen, 2cv, true, classic, deserves, place, motoring, history, originally, designed, cheap, simple, car, use, french, countryside, became, accepted, fashionable, means, transport, city, dwellers, Matt, White, The, Crowood, Press, 4.0, 2005, 200]                                                                                                                                                                                                                                                                                                         |\n",
      "+-------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Id     |Name                                                                         |features                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|4411486|Larousse Dictionary of Beliefs and Religions                                 |(10000,[1,11,16,19,22,59,66,114,133,134,135,152,161,191,216,324,379,440,570,620,731,820,874,889,937,1339,1524,1546,1566,1882,2005,2214,2235,3045,3280,3477,3852,4769,5022,5652,6009,6017,6334,7084,8081,9593],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|4411489|Enhancing Industrial Performance: Experiences Of Integrating The Human Factor|(10000,[1,2,3,7,43,45,76,85,91,172,234,356,389,505,608,1088,1153,1221,1325,1378,1778,2064,2197,2840,3016,4675,8057],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                      |\n",
      "|4411490|The Brass Check: A Study of American Journalism                              |(10000,[3,6,9,10,42,65,118,126,354,381,501,560,580,683,1245,1270,1279,2516,2766,2832,3329,3356,3680,4142,4275,4499,7173],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                 |\n",
      "|4411491|Transient Temperatures in Engineering and Science                            |(10000,[0,2,10,14,68,76,88,90,95,142,156,184,296,331,333,370,377,401,428,526,546,695,714,718,803,978,1012,1845,1899,2166,2279,2477,3272,3796,9224],[1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                       |\n",
      "|4411496|Citroen 2CV: The Complete Story                                              |(10000,[3,17,26,37,103,259,280,360,398,460,478,617,734,768,775,1070,1647,2046,2556,2640,4750,4771,9684],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                  |\n",
      "+-------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tok.select(\"Id\",\"Name\",\"Description\", \"words\", \"filtered_words\").show(5, truncate=50)\n",
    "df_tags.select(\"Id\",\"Name\",\"tags\").show(5, truncate=False)\n",
    "df_feat.select(\"Id\",\"Name\",\"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54af7481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:21:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/06/26 22:21:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar books to Id=4411879:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------+------------------+\n",
      "|Id     |Name                                                                                                        |distCol           |\n",
      "+-------+------------------------------------------------------------------------------------------------------------+------------------+\n",
      "|4411879|Knowing What Works in Health Care: A Roadmap for the Nation                                                 |0.0               |\n",
      "|4490303|Healthy Medicine: A Guide Emergence Sensible. Comprehensive Care                                            |13.114877048604   |\n",
      "|4799451|Introduction to Health Care Delivery: A Primer for Pharmacists                                              |13.228756555322953|\n",
      "|4207719|The Quality Revolution And Health Care: A Primer For Purchasers And Providers                               |13.30413469565007 |\n",
      "|4415234|Health Care Systems in Japan and the United States: A Simulation Study and Policy Analysis                  |13.416407864998739|\n",
      "|4184572|Aging And Health Care: Social Science And Policy Perspectives                                               |13.601470508735444|\n",
      "|4658747|Health Care Systems in Transition: An International Perspective                                             |13.638181696985855|\n",
      "|4219690|Comparative Records for Health Information Management                                                       |13.784048752090222|\n",
      "|4534984|Strategies For Integrated Health Care: Emerging Practices In Information Management And Cross Continuum Care|13.820274961085254|\n",
      "|4213077|Health and Poverty                                                                                          |13.820274961085254|\n",
      "+-------+------------------------------------------------------------------------------------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "# Tune LSH hyperparameters\n",
    "bucket_lengths = [2.0, 3.0]\n",
    "hash_tables    = [4, 6]\n",
    "\n",
    "lsh = BucketedRandomProjectionLSH(\n",
    "    inputCol=\"features\", outputCol=\"hashes\",\n",
    "    bucketLength=2.0,\n",
    "    numHashTables=6\n",
    ")\n",
    "lsh_model = lsh.fit(df_feat)\n",
    "\n",
    "# Example: find top-10 similar books to a given Id\n",
    "target_id  = 4411879\n",
    "target_vec = df_feat.filter(col(\"Id\")==target_id).select(\"features\").first()[0]\n",
    "neighbors  = lsh_model.approxNearestNeighbors(df_feat, target_vec, 10)\n",
    "print(f\"Top 10 similar books to Id={target_id}:\")\n",
    "neighbors.select(\"Id\",\"Name\",\"distCol\").orderBy(\"distCol\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29abb35c",
   "metadata": {},
   "source": [
    "## API Endpoint Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f1530",
   "metadata": {},
   "source": [
    "### /favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7dcdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Converted 183775 books to Pandas DataFrame\n",
      "â–¶ï¸ Creating synthetic user-book interactions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 50,000 synthetic ratings\n",
      "â–¶ï¸ Training ALS collaborative filtering model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:22:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Recommendation models ready!\n"
     ]
    }
   ],
   "source": [
    "# ===== CONTENT-BASED RECOMMENDATIONS =====\n",
    "\n",
    "books_pdf = df_feat.select(\"Id\", \"Name\", \"Authors\", \"Rating\", \"Publisher\", \"Description\", \"features\").toPandas()\n",
    "print(f\"âœ… Converted {len(books_pdf)} books to Pandas DataFrame\")\n",
    "\n",
    "# Content-based similarity using LSH\n",
    "def recommend_similar_books_lsh(book_id, top_n=10):\n",
    "    \"\"\"Find similar books using LSH model\"\"\"\n",
    "    try:\n",
    "        # Get target book features\n",
    "        target_row = df_feat.filter(col(\"Id\") == book_id).first()\n",
    "        if not target_row:\n",
    "            return []\n",
    "        \n",
    "        target_vec = target_row[\"features\"]\n",
    "        \n",
    "        # Find similar books using LSH\n",
    "        neighbors = lsh_model.approxNearestNeighbors(df_feat, target_vec, top_n + 1)\n",
    "        similar_books = neighbors.filter(col(\"Id\") != book_id).select(\"Id\", \"Name\", \"Rating\", \"distCol\").orderBy(\"distCol\").limit(top_n)\n",
    "        \n",
    "        return similar_books.toPandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LSH recommendation: {e}\")\n",
    "        return []\n",
    "\n",
    "# By author recommendations\n",
    "def recommend_by_author(author, top_n=10):\n",
    "    \"\"\"Find books by the same author\"\"\"\n",
    "    return (df_feat.filter(col(\"Authors\").contains(author))\n",
    "                  .orderBy(col(\"Rating\").cast(FloatType()).desc())\n",
    "                  .select(\"Id\", \"Name\", \"Authors\", \"Rating\")\n",
    "                  .limit(top_n)\n",
    "                  .toPandas())\n",
    "\n",
    "# ===== COLLABORATIVE FILTERING (ALS) =====\n",
    "\n",
    "print(\"â–¶ï¸ Creating synthetic user-book interactions...\")\n",
    "\n",
    "# Create synthetic ratings data\n",
    "from pyspark.sql.functions import rand, when\n",
    "import random\n",
    "\n",
    "# Generate synthetic user interactions\n",
    "num_users = 1000\n",
    "num_interactions = 50000\n",
    "\n",
    "# Create synthetic ratings\n",
    "synthetic_ratings = []\n",
    "book_ids = [row.Id for row in df_clean.select(\"Id\").collect()]\n",
    "\n",
    "for _ in range(num_interactions):\n",
    "    user_id = random.randint(1, num_users)\n",
    "    book_id = random.choice(book_ids)\n",
    "    # Bias ratings toward higher ratings (more realistic)\n",
    "    rating = random.choices([1, 2, 3, 4, 5], weights=[5, 10, 20, 35, 30])[0]\n",
    "    synthetic_ratings.append((user_id, book_id, float(rating)))\n",
    "\n",
    "ratings_df = spark.createDataFrame(synthetic_ratings, [\"userId\", \"bookId\", \"rating\"])\n",
    "print(f\"âœ… Created {ratings_df.count():,} synthetic ratings\")\n",
    "\n",
    "# Train ALS model\n",
    "print(\"â–¶ï¸ Training ALS collaborative filtering model...\")\n",
    "als = ALS(userCol=\"userId\", itemCol=\"bookId\", ratingCol=\"rating\",\n",
    "          rank=50, maxIter=10, regParam=0.1, coldStartStrategy=\"drop\")\n",
    "als_model = als.fit(ratings_df)\n",
    "\n",
    "def recommend_for_user(user_id, top_n=10):\n",
    "    try:\n",
    "        user_df = spark.createDataFrame([(user_id,)], [\"userId\"])\n",
    "        recs = als_model.recommendForUserSubset(user_df, top_n)\n",
    "        \n",
    "        if recs.count() > 0:\n",
    "            rec_items = recs.select(\"recommendations\").first()[\"recommendations\"]\n",
    "            book_ids = [item[\"bookId\"] for item in rec_items]\n",
    "            \n",
    "            # Get book details\n",
    "            recommended_books = df_feat.filter(col(\"Id\").isin(book_ids)).select(\"Id\", \"Name\", \"Authors\", \"Rating\").toPandas()\n",
    "            return recommended_books\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error in collaborative filtering: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"âœ… Recommendation models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9fa71",
   "metadata": {},
   "source": [
    "### /popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418a781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "def get_popular_books(k=20):\n",
    "    \"\"\"Get the most popular books sorted by rating - Fixed version\"\"\"\n",
    "    \n",
    "    # Use the existing books_pdf DataFrame but fix the column alignment issue\n",
    "    # The issue was that during toPandas() conversion, columns got misaligned\n",
    "    \n",
    "    df_copy = books_pdf.copy()\n",
    "    \n",
    "    # Convert Rating column to numeric, handling the case where it might contain text\n",
    "    df_copy[\"Rating_Numeric\"] = pd.to_numeric(df_copy[\"Rating\"], errors='coerce')\n",
    "    \n",
    "    # If Rating_Numeric has NaN values, it means the Rating column contains non-numeric data\n",
    "    # In that case, we need to find the correct numeric column\n",
    "    if df_copy[\"Rating_Numeric\"].isna().all():\n",
    "        # Check each column to find the one with numeric rating-like values\n",
    "        for col_name in df_copy.columns:\n",
    "            if col_name != \"Rating\":\n",
    "                try:\n",
    "                    numeric_col = pd.to_numeric(df_copy[col_name], errors='coerce')\n",
    "                    # Check if this column has values that look like ratings (0-5 range)\n",
    "                    if not numeric_col.isna().all() and numeric_col.min() >= 0 and numeric_col.max() <= 5:\n",
    "                        print(f\"âš ï¸  Found rating values in column '{col_name}' instead of 'Rating'\")\n",
    "                        df_copy[\"Rating_Numeric\"] = numeric_col\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # Sort by rating and return top k\n",
    "    result = df_copy.nlargest(k, \"Rating_Numeric\")[[\"Id\", \"Name\", \"Authors\", \"Rating_Numeric\"]]\n",
    "    result = result.rename(columns={\"Rating_Numeric\": \"Rating\"})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7aa5b",
   "metadata": {},
   "source": [
    "### /search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b529849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_books(query, df_pd, top_n=20):\n",
    "    mask = (df_pd[\"Name\"].str.contains(query, case=False, na=False) | \n",
    "            df_pd[\"Description\"].str.contains(query, case=False, na=False))\n",
    "    return df_pd[mask].head(top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701204b",
   "metadata": {},
   "source": [
    "## Model Testing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d2e7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing all recommendation functions...\n",
      "==================================================\n",
      "\n",
      "1. ğŸ“ˆ Testing Popular Books:\n",
      "          Id                                               Name  Rating\n",
      "3    4411491  Transient Temperatures in Engineering and Science     5.0\n",
      "52   4411745                    Mr. Carteret: And Other Stories     5.0\n",
      "84   4411873                  Foggy Moggy Inn (Scaredy Cats S.)     5.0\n",
      "87   4411879  Knowing What Works in Health Care: A Roadmap f...     5.0\n",
      "103  4411935                                    At the Seashore     5.0\n",
      "\n",
      "2. ğŸ” Testing Search Function:\n",
      "            Id                                               Name  \\\n",
      "6356   4114701                                    A Point of View   \n",
      "22571  4143960  The Art Of Investigative Interviewing: A Human...   \n",
      "30420  4366752  Selections from Harry Potter and the Goblet of...   \n",
      "\n",
      "                  Authors  \n",
      "6356          Clive James  \n",
      "22571  Charles L. Yeschke  \n",
      "30420       Patrick Doyle  \n",
      "\n",
      "3. ğŸ‘¤ Testing Author-based Recommendations:\n",
      "Finding books by author: Frank Whaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                          Name        Authors Rating\n",
      "0  4755351              Understanding the Brahma Kumaris  Frank Whaling    2.6\n",
      "1  4411486  Larousse Dictionary of Beliefs and Religions  Frank Whaling    0.0\n",
      "2  4721402        Theory and Method in Religious Studies  Frank Whaling    0.0\n",
      "\n",
      "4. ğŸ“š Testing LSH Content-based Recommendations:\n",
      "Finding books similar to: 'Larousse Dictionary of Beliefs and Religions' (ID: 4411486)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               Name   distCol\n",
      "0  4252279               Diccionario de Santa Teresa de Jesus  7.549834\n",
      "1  4155217      Energy & High Performance Facility Sourcebook  7.549834\n",
      "2  4515226                              Dictionary of Flavors  7.549834\n",
      "3  4022096                                            Sikhism  7.549834\n",
      "4  4107059  Relations Between the European Union and Latin...  7.615773\n",
      "\n",
      "5. ğŸ¤– Testing Collaborative Filtering (ALS):\n",
      "Finding recommendations for user ID: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id                                               Name  \\\n",
      "0  4479884                                 Rescuing Christine   \n",
      "1  4710748                                          Cockatoos   \n",
      "2  4507713  The Golden Age of the Passenger Train: From St...   \n",
      "3  4511424           The Cambridge Companion to Edith Wharton   \n",
      "4  4589050  Dear Walt: Like Moses To Midian, He Left A Pla...   \n",
      "\n",
      "             Authors Rating  \n",
      "0        Alyssa Dean   2.75  \n",
      "1  Werner Lantermann   3.68  \n",
      "2         C.J. Riley    1.5  \n",
      "3     Millicent Bell    4.0  \n",
      "4       R.D. Frazier    5.0  \n",
      "\n",
      "==================================================\n",
      "âœ… All tests completed! Models are ready for deployment.\n",
      "\n",
      "ğŸ“Š Model Summary:\n",
      "   ğŸ“š Total books: 183,775\n",
      "   ğŸ¯ Features dimension: 10,000\n",
      "   ğŸ” LSH hash tables: 6\n",
      "   ğŸ‘¥ ALS rank: 50\n",
      "   ğŸ“ˆ Synthetic ratings: 50,000\n"
     ]
    }
   ],
   "source": [
    "# ===== COMPREHENSIVE MODEL TESTING =====\n",
    "\n",
    "print(\"ğŸ§ª Testing all recommendation functions...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Popular Books\n",
    "print(\"\\n1. ğŸ“ˆ Testing Popular Books:\")\n",
    "popular = get_popular_books(k=5)\n",
    "print(popular[[\"Id\", \"Name\", \"Rating\"]].head())\n",
    "\n",
    "# Test 2: Search Function  \n",
    "print(\"\\n2. ğŸ” Testing Search Function:\")\n",
    "search_results = search_books(\"Harry Potter\", books_pdf, top_n=3)\n",
    "if len(search_results) > 0:\n",
    "    print(search_results[[\"Id\", \"Name\", \"Authors\"]].head())\n",
    "else:\n",
    "    print(\"No Harry Potter books found, trying 'love'...\")\n",
    "    search_results = search_books(\"love\", books_pdf, top_n=3)\n",
    "    print(search_results[[\"Id\", \"Name\", \"Authors\"]].head())\n",
    "\n",
    "# Test 3: Author-based Recommendations\n",
    "print(\"\\n3. ğŸ‘¤ Testing Author-based Recommendations:\")\n",
    "# Get a sample author\n",
    "sample_author = books_pdf[\"Authors\"].iloc[0]\n",
    "print(f\"Finding books by author: {sample_author}\")\n",
    "author_recs = recommend_by_author(sample_author, top_n=3)\n",
    "print(author_recs[[\"Id\", \"Name\", \"Authors\", \"Rating\"]])\n",
    "\n",
    "# Test 4: Content-based Recommendations (LSH)\n",
    "print(\"\\n4. ğŸ“š Testing LSH Content-based Recommendations:\")\n",
    "# Get a random book ID\n",
    "sample_book_id = books_pdf[\"Id\"].iloc[0]\n",
    "sample_book_name = books_pdf[books_pdf[\"Id\"] == sample_book_id][\"Name\"].iloc[0]\n",
    "print(f\"Finding books similar to: '{sample_book_name}' (ID: {sample_book_id})\")\n",
    "\n",
    "similar_books = recommend_similar_books_lsh(sample_book_id, top_n=5)\n",
    "if len(similar_books) > 0:\n",
    "    print(similar_books[[\"Id\", \"Name\", \"distCol\"]])\n",
    "else:\n",
    "    print(\"No similar books found\")\n",
    "\n",
    "# Test 5: Collaborative Filtering (ALS)\n",
    "print(\"\\n5. ğŸ¤– Testing Collaborative Filtering (ALS):\")\n",
    "sample_user_id = 1\n",
    "print(f\"Finding recommendations for user ID: {sample_user_id}\")\n",
    "user_recs = recommend_for_user(sample_user_id, top_n=5)\n",
    "if len(user_recs) > 0:\n",
    "    print(user_recs[[\"Id\", \"Name\", \"Authors\", \"Rating\"]])\n",
    "else:\n",
    "    print(\"No recommendations found for this user\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… All tests completed! Models are ready for deployment.\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nğŸ“Š Model Summary:\")\n",
    "print(f\"   ğŸ“š Total books: {len(books_pdf):,}\")\n",
    "print(f\"   ğŸ¯ Features dimension: {len(cv_model.vocabulary):,}\")\n",
    "print(f\"   ğŸ” LSH hash tables: {lsh.getNumHashTables()}\")\n",
    "print(f\"   ğŸ‘¥ ALS rank: {als.getRank()}\")\n",
    "print(f\"   ğŸ“ˆ Synthetic ratings: {ratings_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5f1e6",
   "metadata": {},
   "source": [
    "## Model Saving & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b0516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving trained models for API deployment...\n",
      "   ğŸ”§ Saving Spark ML models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:23:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/06/26 22:23:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Saving books data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/26 22:23:13 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“‹ Saving additional metadata...\n",
      "âœ… All models saved successfully!\n",
      "ğŸ“ Models saved to: /Users/hazwanadh/Code/Sem4/bigdata/goodreads-book-recommend-bigdata/api-backend/models\n",
      "\n",
      "ğŸ“‹ Saved components:\n",
      "   - als_model/ (Collaborative Filtering)\n",
      "   - lsh_model/ (Content-based Similarity)\n",
      "   - cv_model/ (Text Vectorization)\n",
      "   - books_features.parquet (Books with ML features)\n",
      "   - books_pandas.parquet (Books in Pandas format)\n",
      "   - vocabulary.pkl (Text vocabulary)\n",
      "   - model_metadata.pkl (Model configuration)\n",
      "   - model_loader.py (Helper for loading models in API)\n",
      "\n",
      "ğŸš€ Models are ready for deployment in the Flask API!\n"
     ]
    }
   ],
   "source": [
    "# ===== SAVE MODELS FOR API DEPLOYMENT =====\n",
    "\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "models_dir = \"../api-backend/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ’¾ Saving trained models for API deployment...\")\n",
    "\n",
    "# 1. Save Spark ML models\n",
    "print(\"   ğŸ”§ Saving Spark ML models...\")\n",
    "als_model.write().overwrite().save(f\"{models_dir}/als_model\")\n",
    "lsh_model.write().overwrite().save(f\"{models_dir}/lsh_model\")\n",
    "cv_model.write().overwrite().save(f\"{models_dir}/cv_model\")\n",
    "\n",
    "# 2. Save books data and features as Parquet for fast loading\n",
    "print(\"   ğŸ“Š Saving books data...\")\n",
    "df_feat.write.mode(\"overwrite\").parquet(f\"{models_dir}/books_features.parquet\")\n",
    "\n",
    "# Save books without the features column for Pandas compatibility\n",
    "books_simple_pdf = books_pdf.drop(columns=['features'])\n",
    "books_simple_pdf.to_parquet(f\"{models_dir}/books_pandas.parquet\")\n",
    "\n",
    "# 3. Save additional data structures\n",
    "print(\"   ğŸ“‹ Saving additional metadata...\")\n",
    "import pickle\n",
    "\n",
    "# Save vocabulary for text processing\n",
    "with open(f\"{models_dir}/vocabulary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cv_model.vocabulary, f)\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    \"total_books\": len(books_pdf),\n",
    "    \"feature_dim\": len(cv_model.vocabulary),\n",
    "    \"lsh_hash_tables\": lsh.getNumHashTables(),\n",
    "    \"lsh_bucket_length\": lsh.getBucketLength(),\n",
    "    \"als_rank\": als.getRank(),\n",
    "    \"als_reg_param\": als.getRegParam(),\n",
    "    \"synthetic_ratings_count\": ratings_df.count(),\n",
    "    \"training_date\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(f\"{models_dir}/model_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_metadata, f)\n",
    "\n",
    "print(\"âœ… All models saved successfully!\")\n",
    "print(f\"ğŸ“ Models saved to: {os.path.abspath(models_dir)}\")\n",
    "print(\"\\nğŸ“‹ Saved components:\")\n",
    "print(\"   - als_model/ (Collaborative Filtering)\")\n",
    "print(\"   - lsh_model/ (Content-based Similarity)\")  \n",
    "print(\"   - cv_model/ (Text Vectorization)\")\n",
    "print(\"   - books_features.parquet (Books with ML features)\")\n",
    "print(\"   - books_pandas.parquet (Books in Pandas format)\")\n",
    "print(\"   - vocabulary.pkl (Text vocabulary)\")\n",
    "print(\"   - model_metadata.pkl (Model configuration)\")\n",
    "\n",
    "# Create a simple model loading helper\n",
    "model_loader_code = '''\n",
    "# Model Loading Helper for API\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSHModel, CountVectorizerModel\n",
    "import pandas as pd\n",
    "\n",
    "def load_models(models_dir):\n",
    "    \"\"\"Load all trained models for the API\"\"\"\n",
    "    \n",
    "    # Initialize Spark\n",
    "    spark = SparkSession.builder.appName(\"GoodreadsAPI\").getOrCreate()\n",
    "    \n",
    "    # Load Spark models\n",
    "    als_model = ALSModel.load(f\"{models_dir}/als_model\")\n",
    "    lsh_model = BucketedRandomProjectionLSHModel.load(f\"{models_dir}/lsh_model\")\n",
    "    cv_model = CountVectorizerModel.load(f\"{models_dir}/cv_model\")\n",
    "    \n",
    "    # Load data\n",
    "    books_df = spark.read.parquet(f\"{models_dir}/books_features.parquet\")\n",
    "    books_pdf = pd.read_parquet(f\"{models_dir}/books_pandas.parquet\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(f\"{models_dir}/model_metadata.pkl\", \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "    return {\n",
    "        \"spark\": spark,\n",
    "        \"als_model\": als_model,\n",
    "        \"lsh_model\": lsh_model,\n",
    "        \"cv_model\": cv_model,\n",
    "        \"books_df\": books_df,\n",
    "        \"books_pdf\": books_pdf,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "'''\n",
    "\n",
    "with open(f\"{models_dir}/model_loader.py\", \"w\") as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "print(\"   - model_loader.py (Helper for loading models in API)\")\n",
    "print(\"\\nğŸš€ Models are ready for deployment in the Flask API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da817630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Final verification: Testing if saved models can be loaded...\n",
      "âœ… Model loader script is valid\n",
      "âœ… Model metadata loaded: 183,775 books\n",
      "âœ… Vocabulary loaded: 10,000 terms\n",
      "âœ… Pandas books data loaded: 183,775 books\n",
      "\n",
      "ğŸ‰ The model is ready for deployment!\n",
      "\n",
      "ğŸ“Š Final Statistics:\n",
      "   ğŸ”¢ Total books processed: 183,775\n",
      "   ğŸ§  ML features dimension: 10,000\n",
      "   ğŸ¯ LSH hash tables: 6\n",
      "   â­ ALS model rank: 50\n",
      "   ğŸ“ˆ Training ratings: 50,000\n",
      "   ğŸ“… Training completed: 2025-06-26T22:23:28.510503\n",
      "\n",
      "ğŸš€ Next steps:\n",
      "   1. Update the Flask API to use these models\n",
      "   2. Deploy the API with model loading\n",
      "   3. Connect the React frontend to the API\n",
      "   4. Test end-to-end recommendations\n"
     ]
    }
   ],
   "source": [
    "# ===== FINAL VERIFICATION =====\n",
    "\n",
    "print(\"ğŸ” Final verification: Testing if saved models can be loaded...\")\n",
    "\n",
    "# Test loading the model loader\n",
    "import sys\n",
    "sys.path.append(\"../api-backend/models\")\n",
    "\n",
    "try:\n",
    "    exec(open(\"../api-backend/models/model_loader.py\").read())\n",
    "    print(\"âœ… Model loader script is valid\")\n",
    "    \n",
    "    # Test loading metadata\n",
    "    import pickle\n",
    "    with open(\"../api-backend/models/model_metadata.pkl\", \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "    print(f\"âœ… Model metadata loaded: {metadata['total_books']:,} books\")\n",
    "    \n",
    "    # Test loading vocabulary\n",
    "    with open(\"../api-backend/models/vocabulary.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(f\"âœ… Vocabulary loaded: {len(vocab):,} terms\")\n",
    "    \n",
    "    # Test loading Pandas data\n",
    "    import pandas as pd\n",
    "    books_test = pd.read_parquet(\"../api-backend/models/books_pandas.parquet\")\n",
    "    print(f\"âœ… Pandas books data loaded: {len(books_test):,} books\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ The model is ready for deployment!\")\n",
    "    print(\"\\nğŸ“Š Final Statistics:\")\n",
    "    print(f\"   ğŸ”¢ Total books processed: {metadata['total_books']:,}\")\n",
    "    print(f\"   ğŸ§  ML features dimension: {metadata['feature_dim']:,}\")\n",
    "    print(f\"   ğŸ¯ LSH hash tables: {metadata['lsh_hash_tables']}\")\n",
    "    print(f\"   â­ ALS model rank: {metadata['als_rank']}\")\n",
    "    print(f\"   ğŸ“ˆ Training ratings: {metadata['synthetic_ratings_count']:,}\")\n",
    "    print(f\"   ğŸ“… Training completed: {metadata['training_date']}\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Next steps:\")\n",
    "    print(\"   1. Update the Flask API to use these models\")\n",
    "    print(\"   2. Deploy the API with model loading\")\n",
    "    print(\"   3. Connect the React frontend to the API\")\n",
    "    print(\"   4. Test end-to-end recommendations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during verification: {e}\")\n",
    "    print(\"Please check the model saving process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
